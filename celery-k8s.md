# Celery en Kubernetes: La Gu√≠a Completa

## üìã Tabla de Contenidos

- [M√≥dulo 1: La Anatom√≠a Real](#m√≥dulo-1-la-anatom√≠a-real-lo-que-nadie-explica)
  - [El Mito del "Worker"](#1-el-mito-del-worker)
  - [El Problema de los Mega Pods](#el-problema-de-los-mega-pods)
  - [El Prefetching](#2-el-prefetching-el-asesino-silencioso-de-performance)
- [M√≥dulo 2: Gesti√≥n de Recursos y Memoria](#m√≥dulo-2-gesti√≥n-de-recursos-y-memoria)
  - [La soluci√≥n Nuclear: max_tasks_per_child](#1-la-soluci√≥n-nuclear-max_tasks_per_child)
  - [La soluci√≥n por L√≠mite: max_memory_per_child](#2-la-soluci√≥n-por-l√≠mite-max_memory_per_child)
- [M√≥dulo 3: Tipos de Pools](#m√≥dulo-3-tipos-de-pools-la-clave-de-la-optimizaci√≥n)
  - [Comparaci√≥n de Tipos de Pool](#comparaci√≥n-de-tipos-de-pool)
  - [Tu optimizaci√≥n en K8s](#tu-optimizaci√≥n-en-k8s)
- [M√≥dulo 4: Estrategia de Dise√±o en Kubernetes](#m√≥dulo-4-estrategia-de-dise√±o-en-kubernetes)
  - [Patr√≥n 1: Split de Colas](#patr√≥n-1-split-de-colas-obligatorio)
  - [Patr√≥n 2: Fair Dispatch](#patr√≥n-2-fair-dispatch-distribuci√≥n-justa)
- [Resumen de Configuraci√≥n Optimizada](#resumen-de-configuraci√≥n-optimizada)

---

## M√≥dulo 1: La Anatom√≠a Real (Lo que nadie explica)

Celery no es solo "enviar tareas". Es un **sistema de tuber√≠as**. Si una tuber√≠a es ancha y la otra angosta, todo explota.

### 1. El Mito del "Worker"

Cuando dices `celery worker`, no lanzas un proceso. Lanzas un **Supervisor** que a su vez lanza un **[Pool de Ejecuci√≥n](pool_connections.md)**.

### El Problema de los Mega Pods

Si en Kubernetes le das a un Pod 10 CPUs y no configuras nada, Celery por defecto usa el modo **prefork** y lanza **10 sub-procesos Python independientes**.

**Consecuencia:** Cada proceso carga en RAM toda tu aplicaci√≥n (Django/FastAPI + librer√≠as).

> ‚ö†Ô∏è **Resultado:** Si tu app pesa 200MB en RAM, ese Pod consume:
> 
> $$200 \text{ MB} \times 10 = 2 \text{ GB}$$
> 
> Solo por arrancar, sin procesar nada.

---

### 2. El Prefetching (El asesino silencioso de performance)

Por defecto, Celery es "ego√≠sta". Un worker intenta agarrar **4 tareas por adelantado** (multiplicado por el n√∫mero de procesos).

#### Escenario Problem√°tico

Tienes **2 workers**. Lanzas **1 tarea pesada** (10 min) y **100 tareas livianas** (1 seg).

**Problema:** El Worker A agarra la pesada... ¬°y se reserva otras 3 livianas para despu√©s! Esas 3 tareas livianas se quedan bloqueadas en la memoria del Worker A esperando a que termine la pesada, mientras el Worker B est√° libre y muerto de risa.

> üö® **S√≠ntoma en K8s:** Ves un pod al 100% CPU y otros al 0%, pero la cola no avanza.

---

## M√≥dulo 2: Gesti√≥n de Recursos y Memoria 

Python no libera la memoria al sistema operativo inmediatamente (fragmentaci√≥n). En procesos de larga duraci√≥n como Celery, esto parece un "Memory Leak", pero es comportamiento normal de Python.

### 1. La soluci√≥n Nuclear: `max_tasks_per_child`

Nunca conf√≠es en que el **[Garbage Collector](garbage_collector.md)** de Python sea perfecto.

**Concepto:** Le dices al worker: "Procesa 1000 tareas y luego mu√©rete".

**Efecto:** El proceso principal (Supervisor) mata al sub-proceso viejo y crea uno nuevo, limpio, con 0 RAM extra.

**Config:**

```bash
--max-tasks-per-child=1000
```

O menos si tienes leaks muy agresivos.

### 2. La soluci√≥n por L√≠mite: `max_memory_per_child`

M√°s segura para K8s. Si un worker supera X cantidad de RAM, se reinicia autom√°ticamente.

**Uso:** Evita que el OOMKiller de Kubernetes mate todo el Pod. Es preferible que Celery reinicie un sub-proceso controladamente a que K8s mate el pod entero bruscamente.

---

## M√≥dulo 3: Tipos de Pools (La clave de la optimizaci√≥n)

Aqu√≠ es donde optimizas "Mega Pods". Tienes que elegir el pool seg√∫n tu tarea.

### Comparaci√≥n de Tipos de Pool

| Tipo de Pool | C√≥mo funciona | √ösalo para... | No lo uses para... |
|--------------|---------------|---------------|---------------------|
| **Prefork** (Default) | 1 Proceso por Tarea | C√°lculo pesado (CPU bound), Procesamiento de im√°genes, Pandas/Numpy. | Peticiones HTTP, I/O, esperar bases de datos. |
| **Gevent / Eventlet** | Green Threads (Miles por proceso) | Network I/O, Web Scraping, llamar APIs externas, enviar emails. | C√°lculos matem√°ticos (bloquean todo el pool). |

### Tu optimizaci√≥n en K8s

**Si tus tareas son llamar a APIs externas o esperar DB:**

- ‚ùå **NO** uses Prefork
- ‚úÖ Usa **gevent**
- üí° Podr√°s manejar **1000 tareas concurrentes** con 1 sola CPU y poca RAM

**Si tus tareas son CPU intensivas:**

- ‚úÖ Usa **Prefork**, pero no hagas Mega Pods
- üí° Haz muchos pods **peque√±os** (1-2 CPUs)
- üìà Es m√°s f√°cil para el Autoscaler de K8s escalar 50 pods peque√±os que 1 pod gigante

---

## M√≥dulo 4: Estrategia de Dise√±o en Kubernetes

No trates a Celery como una caja negra. Div√≠delo.

### Patr√≥n 1: Split de Colas (Obligatorio)

Nunca mezcles tareas r√°pidas con tareas lentas en la misma cola/worker.

- **Queue `default`:** Tareas r√°pidas (emails, notificaciones)
- **Queue `heavy`:** Reportes, procesamiento de video

#### En Kubernetes, tendr√≠as Deployments separados:

- **`deployment-worker-default`:** Escala por CPU o n√∫mero de mensajes
- **`deployment-worker-heavy`:** Escala diferente, quiz√°s con nodos Spot de AWS

### Patr√≥n 2: Fair Dispatch (Distribuci√≥n Justa)

Para arreglar el problema de carga desbalanceada:

**`task_acks_late = True`:** La tarea solo se borra de RabbitMQ cuando termina. Si el worker muere a la mitad, RabbitMQ la reenv√≠a a otro.

**`worker_prefetch_multiplier = 1`:** El worker nunca reserva tareas. Toma 1, la termina, y va por la siguiente.

> üìù **Nota:** Esto baja un poco el rendimiento (m√°s viajes a RabbitMQ) pero garantiza que ninguna tarea se quede "secuestrada" en un worker lento.

---

## Resumen de Configuraci√≥n Optimizada

Para tu pr√≥ximo despliegue (primero Docker Compose, luego K8s), esta es tu "Biblia de Configuraci√≥n":

```python
# celeryconfig.py

# 1. Evitar que tareas largas bloqueen tareas cortas (Fair Dispatch)
task_acks_late = True
worker_prefetch_multiplier = 1 

# 2. Controlar Memoria (Anti-Leaks)
worker_max_tasks_per_child = 500  # Reiniciar proceso cada 500 tareas
worker_max_memory_per_child = 150000  # Reiniciar si usa m√°s de 150MB (ajustar a tu pod)

# 3. Timeouts (Para que no se queden colgadas forever)
task_time_limit = 1800       # Kill -9 a los 30 min
task_soft_time_limit = 1500  # Exception a los 25 min (da tiempo a limpiar)

# 4. Serializaci√≥n (Seguridad y peso)
task_serializer = 'json'
result_serializer = 'json'
accept_content = ['json']
```
